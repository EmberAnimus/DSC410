{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preface\n",
    "Before I launch into the code I want to preface a misunderstanding I had about my data initially. I originally thought that the ` weighted_vote_score ` was a measure of the impact it had on the games review score. As it is instead a representation of the gauged helpfulness of the community, I now need to somewhat change my approach. \n",
    "Instead of focusing on attempting to guess within a range of the previous target parameter `weighted_vote_score` I'll instead need to approach it as attempting to guess 'Yes' or 'No' for whether or not they recommend the game (` voted_up `). \n",
    "\n",
    "# General Approach\n",
    "As a whole I'll need to somewhat adjust the values in my data. I'll change the boolean values into 0 or 1 respectively, and drop the language column (since it appears to be entirely english). From here I'll only include the recieved_for_free, written_during_early_acces, weighted_vote_score, and review in the algorithm. This will then be fed into the algorithm to generate a guess on the voted_up scores. \n",
    "\n",
    "# Transforming the review column\n",
    "Ultimately I need a way to perform sentiment analysis of the reviews to convert it from text into a numerical value of positivity or negativity. This in account with the other factors should (hopefully) get us a good guess of whether or not someone would recommend a game. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sebas\\AppData\\Local\\Temp\\ipykernel_12652\\1799129055.py:9: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  filtered_df['review'] = filtered_df['review'].str.replace(r'\\W+', ' ')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>recommendationid</th>\n",
       "      <th>voted_up</th>\n",
       "      <th>received_for_free</th>\n",
       "      <th>written_during_early_access</th>\n",
       "      <th>weighted_vote_score</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70</td>\n",
       "      <td>115513013</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.729283</td>\n",
       "      <td>Review of Half Life Revolutionizing the indus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70</td>\n",
       "      <td>115813617</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>A must play classic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>115817244</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>One of the best games every created still fun ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>115566933</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.613007</td>\n",
       "      <td>sp is pretty cool deathmatch goes crazy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70</td>\n",
       "      <td>116146745</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>I m Kayne West and this is the Kayne best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>70</td>\n",
       "      <td>116216878</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>70</td>\n",
       "      <td>115547553</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.537176</td>\n",
       "      <td>I ve come to make an announcement Gordon Freem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>70</td>\n",
       "      <td>115619966</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.527528</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>70</td>\n",
       "      <td>116243034</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.526959</td>\n",
       "      <td>OMFG BEST GRAPHICSSSSSSSSSSSSSS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>70</td>\n",
       "      <td>115766562</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.525862</td>\n",
       "      <td>very noice game</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ids  recommendationid  voted_up  received_for_free  \\\n",
       "0   70         115513013         1                  0   \n",
       "1   70         115813617         1                  0   \n",
       "2   70         115817244         1                  0   \n",
       "3   70         115566933         1                  0   \n",
       "4   70         116146745         1                  0   \n",
       "5   70         116216878         1                  0   \n",
       "6   70         115547553         0                  0   \n",
       "7   70         115619966         1                  0   \n",
       "8   70         116243034         1                  0   \n",
       "9   70         115766562         1                  0   \n",
       "\n",
       "   written_during_early_access  weighted_vote_score  \\\n",
       "0                            0             0.729283   \n",
       "1                            0             0.642857   \n",
       "2                            0             0.615385   \n",
       "3                            0             0.613007   \n",
       "4                            0             0.583333   \n",
       "5                            0             0.565217   \n",
       "6                            0             0.537176   \n",
       "7                            0             0.527528   \n",
       "8                            0             0.526959   \n",
       "9                            0             0.525862   \n",
       "\n",
       "                                              review  \n",
       "0   Review of Half Life Revolutionizing the indus...  \n",
       "1                               A must play classic   \n",
       "2  One of the best games every created still fun ...  \n",
       "3            sp is pretty cool deathmatch goes crazy  \n",
       "4          I m Kayne West and this is the Kayne best  \n",
       "5                                               1998  \n",
       "6  I ve come to make an announcement Gordon Freem...  \n",
       "7                                               Yes   \n",
       "8                   OMFG BEST GRAPHICSSSSSSSSSSSSSS   \n",
       "9                                    very noice game  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "base_path = Path()\n",
    "raw_data = base_path.joinpath('raw_data')\n",
    "filtered_data = 'filtered_data.json'\n",
    "\n",
    "filtered_df = pd.read_json(raw_data.joinpath(filtered_data))\n",
    "filtered_df['review'] = filtered_df['review'].str.replace(r'\\W+', ' ')\n",
    "filtered_df[['voted_up', 'received_for_free', 'written_during_early_access']] = filtered_df[['voted_up', 'received_for_free', 'written_during_early_access']].astype(int)\n",
    "filtered_df.drop(columns='language', inplace=True, axis=1)\n",
    "display(filtered_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25      80.00\n",
      "0.50     238.00\n",
      "0.75     613.00\n",
      "0.90    1347.00\n",
      "0.95    2088.35\n",
      "0.99    4310.00\n",
      "Name: review, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#We filter this to cut down on the amount I have to pad. I can capture ~90% of the data by cutting it off at 1350 characters.\n",
    "#With just slicing the first 100 reviews I was able to go from ~9min down to 30 seconds. \n",
    "#I've tried experimenting with some of the methods to speed up the process, but I haven't been able to get much to work without something else breaking.\n",
    "\n",
    "print(filtered_df['review'].apply(len).quantile([0.25, 0.5, 0.75, 0.9, 0.95, 0.99]))\n",
    "filtered_df = filtered_df.loc[filtered_df['review'].apply(len) < 1350]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "This portion of the process has been somewhat of a struggle. While I could use NLTK or similar libaries to maybe get a good guess at the sentiment values, I want to test if a Neural Network model can get resonably close as well. First I had to figure out how to shape the data in a way that the Neural Network would properly work with, via subclassing the Dataset class, then I had to define various methods to make the data work.\n",
    "\n",
    "Right now the Dataset takes too long to process, at least in order for me to submit the assignment in at a resonable time, however I will improve the effciency of this such that we can get a good test of the neural network benefits.\n",
    "\n",
    "After a couple days of working out the issues in the Dataset subclass, I was able to have a working model of the data to plug into the CNN Sentiment Analysis model. From what I could find CNN models worked reasonably well at learning the sentiments of datasets as well as long-term patterns. So I opted to utilize this method and implement a model to guess the data.\n",
    "\n",
    "This process took a while to get it right, but finally I was left with my current model. The inital tests with a limited selection showed to be promising whe compared to the test set - but against the total dataframe it just guessed everything as being a recommended review - which isn't ideal. At the moment I'm going to assume its from having too many unexpected tokens - resulting in poor performance, but it's hard to say until I can train it against the whole of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0040013790130615234,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2673e3e0900473693bed13639f2fa74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0029985904693603516,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 200,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a944b66c81a411583fbf7a7b7b637c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_dataframe, test_dataframe = train_test_split(filtered_df[['review', 'voted_up']], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.tokenizer = torchtext.data.get_tokenizer('basic_english')\n",
    "        self.max_seq_len = self._infer_max_seq_len()\n",
    "        self.vocab = self._build_vocab()\n",
    "        self.numericalized_data = self._numericalize_data()\n",
    "\n",
    "    def _infer_max_seq_len(self):\n",
    "        max_seq_len = 0\n",
    "        for text in self.df['review']:\n",
    "            tokens = self.tokenizer(text)\n",
    "            max_seq_len = max(max_seq_len, len(tokens))\n",
    "        return max_seq_len\n",
    "\n",
    "    def _build_vocab(self):\n",
    "        tokenized_reviews = []\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            for tokens in executor.map(self.tokenizer, self.df['review']):\n",
    "                tokenized_reviews.append(tokens)\n",
    "\n",
    "        vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_reviews, specials=['<unk>', '<pad>'])\n",
    "        vocab.set_default_index(vocab['<unk>'])\n",
    "        return vocab\n",
    "\n",
    "    def _numericalize_data(self):\n",
    "            tokenized_reviews = [self.tokenizer(text) for text in self.df['review']]\n",
    "            numericalized_reviews = np.zeros((len(tokenized_reviews), self.max_seq_len), dtype=np.int64)\n",
    "            for i, tokens in tqdm(enumerate(tokenized_reviews), total=len(tokenized_reviews)):\n",
    "                numericalized_tokens = [self.vocab.get_stoi()[token] for token in tokens]\n",
    "                numericalized_reviews[i, :len(numericalized_tokens)] = numericalized_tokens[:self.max_seq_len]\n",
    "            return torch.from_numpy(numericalized_reviews), torch.tensor(self.df['voted_up'].values, dtype=torch.long)\n",
    "\n",
    "    def _pad_tokens(self, tokens):\n",
    "        num_tokens = len(tokens)\n",
    "        if num_tokens < self.max_seq_len:\n",
    "            padded_tokens = tokens + ['<pad>'] * (self.max_seq_len - num_tokens)\n",
    "        else:\n",
    "            padded_tokens = tokens[:self.max_seq_len]\n",
    "        return padded_tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.numericalized_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        numericalized_review = self.numericalized_data[0][idx]\n",
    "        label = self.numericalized_data[1][idx]\n",
    "        return numericalized_review, label\n",
    "\n",
    "    def get_vocab(self, index=False):\n",
    "        '''Returns the vocab object as number index if true, else as string index'''\n",
    "        return self.vocab.get_itos() if index else self.vocab.get_stoi()\n",
    "\n",
    "\n",
    "\n",
    "train_data = CustomDataset(train_dataframe[:1000])\n",
    "test_data = CustomDataset(test_dataframe[:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([28, 123, 148, 27, 287, 538, 408, 412, 298, 576, 5045, 3028, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], tensor(1.))\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class SentimentCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embedding_dim,\n",
    "                      out_channels=n_filters,\n",
    "                      kernel_size=fsz)\n",
    "            for fsz in filter_sizes\n",
    "        ])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text = [batch size, sent len]\n",
    "        embedded = self.embedding(text)\n",
    "        # embedded = [batch size, sent len, emb dim]\n",
    "        embedded = embedded.permute(0, 2, 1)  # [batch size, emb dim, sent len]\n",
    "\n",
    "        # apply convolutions and activation functions\n",
    "        conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
    "\n",
    "        # pooling\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "\n",
    "        # concatenate pooled features and pass through the dropout layer\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "\n",
    "        # pass through the fully connected layer\n",
    "        out = self.fc(cat)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.3514\n",
      "Epoch: 2, Loss: 0.3498\n",
      "Epoch: 3, Loss: 0.0086\n",
      "Epoch: 4, Loss: 0.0092\n",
      "Epoch: 5, Loss: 0.0022\n",
      "Epoch: 6, Loss: 0.0017\n",
      "Epoch: 7, Loss: 0.0014\n",
      "Epoch: 8, Loss: 0.0003\n",
      "Epoch: 9, Loss: 0.0002\n",
      "Epoch: 10, Loss: 0.0002\n",
      "Accuracy on test set: 100.00% \n",
      " R2 Score: 100.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import r2_score\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=True)\n",
    "\n",
    "vocab_size = len(train_data.vocab)\n",
    "emb_dim = 100\n",
    "num_filters = 100\n",
    "filter_sizes = [2, 3, 4]\n",
    "output_dim = 2\n",
    "dropout = 0.5\n",
    "\n",
    "model = SentimentCNN(vocab_size, emb_dim, num_filters, filter_sizes, output_dim, dropout)\n",
    "\n",
    "# define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# train the model\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for batch in train_data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        text, label = batch\n",
    "        output = model(text)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print('Epoch: {}, Loss: {:.4f}'.format(epoch+1, running_loss/len(train_data_loader)))\n",
    "    \n",
    "# evaluate the model on test set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_data_loader:\n",
    "        text, label = batch\n",
    "        output = model(text)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "print(f'Accuracy on test set: {accuracy_score(label,predicted)*100:.2f}% \\n R2 Score: {r2_score(label, predicted)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), raw_data.joinpath('model.pt'))\n",
    "torch.save(train_data.get_vocab(), raw_data.joinpath('vocab.pt'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment predictions\n",
    "My overall approach here is to get the sentiment values of each word by training the model to correctly guess the `voted_up` field and develop the necessary coeffecients to do so. \n",
    "\n",
    "The process to extract their coeffients involves the following steps:\n",
    "   1. The input text is first tokenized using the basic_english tokenizer from torchtext.\n",
    "   1.  The tokenized text is then converted to numerical form by mapping each token to its corresponding index in the vocabulary.\n",
    "   1.  The numericalized tokens are then padded to a fixed length to ensure that all inputs have the same shape.\n",
    "   1.  The padded numericalized tokens are passed through the embedding layer of the model to obtain their corresponding word embeddings.\n",
    "   1.  The mean embedding for each review is calculated by taking the element-wise product of the embedding tensor and a binary mask tensor (which is 1 for tokens that are present and 0 for tokens that are padded) and then taking the mean along the sequence dimension.\n",
    "   1.  The mean embeddings are passed through a 1D convolutional layer (model.convs[0]), followed by a non-linear activation function (torch.tanh).\n",
    "   1.  The output of the convolutional layer is a 3D tensor with shape (batch_size, num_filters, seq_len - filter_size + 1).\n",
    "   1.  The tensor is then squeezed along the third dimension (i.e., `output = output.squeeze(3)`), resulting in a 2D tensor with shape ( `batch_size, num_filters, seq_len - filter_size + 1`).\n",
    "   1.  The tensor is then averaged along the third dimension (i.e., `output = output.mean(dim=2)`), resulting in a 2D tensor with shape (`batch_size, num_filters`).\n",
    "   1.  The tensor is then squeezed along the first dimension (i.e., `output = output.squeeze()`), resulting in a 1D tensor with shape (`num_filters,`).\n",
    "   1.  The final sentiment score for each review is obtained by taking the hyperbolic tangent of the average of the values in the 1D tensor (i.e., `torch.tanh(output.mean())`). This forces the value between -1 and 1, resulting in the actual sentiment score of the word in context to the sentence.\n",
    "\n",
    "Then I take the resulting lists, and iteratively strip the NaN values from them, this will allow me to more effeciently process the list later once it is inside of the dataframe.\n",
    "Finally in the dataframe I take the average of the values of the words in the sentences to get the final sentiment score of the sentence, dropping the last set of NaN values for sentences that we can't produce a score for (due to lack of vocab across the sentence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(df):\n",
    "\n",
    "    # Tokenize the review text\n",
    "    tokenizer = torchtext.data.get_tokenizer('basic_english')\n",
    "    temp_df = df['review'].apply(tokenizer)\n",
    "    vocab = train_data.get_vocab()\n",
    "\n",
    "    def numericalize_token(token):\n",
    "        if token not in vocab:\n",
    "            return vocab['<unk>']\n",
    "        else:\n",
    "            return vocab[token]\n",
    "\n",
    "    # Convert the tokens to numericalized form\n",
    "    numericalized_tokens = []\n",
    "    for tokens in tqdm(temp_df, desc='Numericalizing tokens'):\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            numericalized_tokens.append(list(executor.map(numericalize_token, tokens)))\n",
    "\n",
    "    # Pad the numericalized tokens to a fixed length\n",
    "    max_seq_len = train_data._infer_max_seq_len()\n",
    "    padded_tokens = torch.zeros((len(numericalized_tokens), max_seq_len), dtype=torch.long)\n",
    "    for i, tokens in tqdm(enumerate(numericalized_tokens), desc='Padding tokens', total=len(numericalized_tokens)):\n",
    "        tokens = tokens[:max_seq_len]\n",
    "        padded_tokens[i, :len(tokens)] = torch.tensor(tokens)\n",
    "\n",
    "    padded_tokens = padded_tokens.unsqueeze(1)\n",
    "\n",
    "    # Calculate the sentiment score based on the mean value of the word embeddings\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        model.convs.eval()\n",
    "        embeddings = model.embedding(padded_tokens)\n",
    "        mask = padded_tokens.ne(0).unsqueeze(-1).float()\n",
    "        mean_embeddings = embeddings.mul(mask).sum(dim=1).div(mask.sum(dim=1))\n",
    "        mean_embeddings = mean_embeddings.permute(0, 2, 1)\n",
    "        output = model.convs[0](mean_embeddings)\n",
    "        output = output.squeeze(2)\n",
    "        output = output.mean(dim=1)\n",
    "        output = output.view(output.shape[0], -1)\n",
    "        sentiment_scores = torch.tanh(output).squeeze().tolist()\n",
    "\n",
    "    # Return the sentiment scores as a new column in the dataframe\n",
    "    sentiments = []\n",
    "    for review_sentiment in tqdm(sentiment_scores, desc='Calculating Sentiment'):\n",
    "        review_sentiment = np.array(review_sentiment, dtype=np.float32)\n",
    "        review_sentiment = [word_sentiment for word_sentiment in review_sentiment if np.isnan(word_sentiment) == False]\n",
    "        sentiments.append(review_sentiment)\n",
    "    return sentiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004004478454589844,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Numericalizing tokens",
       "rate": null,
       "total": 1000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "173163c1ab7548b38de1a5cb8dbd976f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Numericalizing tokens:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009002685546875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Padding tokens",
       "rate": null,
       "total": 1000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734f2bc9babc43bfbb89a435520b0f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Padding tokens:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006005287170410156,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Calculating Sentiment",
       "rate": null,
       "total": 1000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae4fda2bb27b48afaf0ecf868a4cdd45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating Sentiment:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_df = pd.DataFrame([filtered_df['review'][:1000].values, predict_sentiment(filtered_df[:1000])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>Internet cafeteria in early 2000s simulator</td>\n",
       "      <td>0.128682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>Hunting Horn Best Weapon</td>\n",
       "      <td>0.101190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>sometime it make poop pant</td>\n",
       "      <td>0.099908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>game for bisexuals</td>\n",
       "      <td>0.099109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>762</th>\n",
       "      <td>i glory kill babies in real life</td>\n",
       "      <td>0.091737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>add sex</td>\n",
       "      <td>-0.105851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>i love sushi 3</td>\n",
       "      <td>-0.109314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>I love peas Me</td>\n",
       "      <td>-0.109314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>GIVE US HL3</td>\n",
       "      <td>-0.121364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>hard like me</td>\n",
       "      <td>-0.129644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>882 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                0         1\n",
       "786  Internet cafeteria in early 2000s simulator   0.128682\n",
       "811                      Hunting Horn Best Weapon  0.101190\n",
       "619                    sometime it make poop pant  0.099908\n",
       "149                            game for bisexuals  0.099109\n",
       "762              i glory kill babies in real life  0.091737\n",
       "..                                            ...       ...\n",
       "637                                       add sex -0.105851\n",
       "746                               i love sushi 3  -0.109314\n",
       "749                                I love peas Me -0.109314\n",
       "342                                   GIVE US HL3 -0.121364\n",
       "722                                  hard like me -0.129644\n",
       "\n",
       "[882 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "count    882.000000\n",
       "mean      -0.001206\n",
       "std        0.023673\n",
       "min       -0.129644\n",
       "25%       -0.009231\n",
       "50%       -0.000606\n",
       "75%        0.008741\n",
       "max        0.128682\n",
       "Name: 1, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_df = temp_df.transpose()\n",
    "temp_df[1] = temp_df[1].apply(lambda x: np.mean(np.array(x)))\n",
    "temp_df.dropna(inplace=True)\n",
    "display(temp_df.sort_values(1, ascending=False))\n",
    "display(temp_df[1].describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In review\n",
    "So in review I have made good strides in accomplishing part of my goal, and generating features for the dataset. However my downfall is a tandem of the effeciency of the code, and not performing more cleaning steps on the text (i.e not converting contractions to their long-form before stripping punctuation like apostrophes). Once some more of these steps are done and I am able to process the full dataset for training - I think the overall performance of the model will improve drastically. I definitely still have a lot more to learn about PyTorch and its associated libraries, but I am somewhat happy with how far I have come."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
